{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Descriptive analysis for the manuscript\n",
    "\n",
    "Summarize geotagged tweets of the multiple regions used for the experiment and the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load_ext autoreload\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import scipy.stats as stats\n",
    "from  tqdm import tqdm\n",
    "\n",
    "def load_region_tweets(region=None):\n",
    "    df = pd.read_csv(f'../../dbs/{region}/geotweets.csv')\n",
    "    df['day'] = df['createdat'].apply(lambda x: x.split(' ')[0])\n",
    "    df['createdat'] = pd.to_datetime(df['createdat'], infer_datetime_format=True)\n",
    "    t_max, t_min = df.createdat.max(), df.createdat.min()\n",
    "    time_span = f'{t_min} - {t_max}'\n",
    "    num_users = len(df.userid.unique())\n",
    "    num_geo = len(df)\n",
    "    num_days = np.median(df.groupby(['userid'])['day'].nunique())\n",
    "    num_geo_freq = np.median(df.groupby(['userid']).size() / df.groupby(['userid'])['day'].nunique())\n",
    "    return region, time_span, num_users, num_geo, num_days, num_geo_freq\n",
    "\n",
    "def user_stats_cal(data):\n",
    "    time_span = data.createdat.max() - data.createdat.min()\n",
    "    time_span = time_span.days\n",
    "    if time_span == 0:\n",
    "        time_span += 1\n",
    "    num_days = data['day'].nunique()\n",
    "    num_geo = len(data)\n",
    "    geo_freq = num_geo / num_days\n",
    "    share_active = num_days / time_span\n",
    "    return pd.DataFrame.from_dict({'time_span': [time_span],\n",
    "            'num_days': [num_days],\n",
    "            'num_geo': [num_geo],\n",
    "            'geo_freq': [geo_freq],\n",
    "            'share_active': [share_active]\n",
    "            })\n",
    "\n",
    "def region_tweets_stats_per_user(region=None):\n",
    "    df = pd.read_csv(f'../../dbs/{region}/geotweets.csv')\n",
    "    df['day'] = df['createdat'].apply(lambda x: x.split(' ')[0])\n",
    "    df['createdat'] = pd.to_datetime(df['createdat'], infer_datetime_format=True)\n",
    "    tqdm.pandas(desc=region)\n",
    "    df_users = df.groupby('userid').progress_apply(user_stats_cal).reset_index()\n",
    "    df_users.loc[:, 'region'] = region\n",
    "    df_users.drop(columns=['level_1'], inplace=True)\n",
    "    return df_users\n",
    "\n",
    "region_list = ['sweden', 'netherlands', 'saopaulo', 'australia', 'austria', 'barcelona',\n",
    "               'capetown', 'cebu', 'egypt', 'guadalajara', 'jakarta',\n",
    "               'johannesburg', 'kualalumpur', 'lagos', 'madrid', 'manila', 'mexicocity', 'moscow', 'nairobi',\n",
    "               'rio', 'saudiarabia', 'stpertersburg', 'surabaya']\n",
    "\n",
    "with open('../../lib/regions.yaml', encoding='utf8') as f:\n",
    "    region_manager = yaml.load(f, Loader=yaml.FullLoader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Summarize the geotagged tweets used as input to the model\n",
    "Geotagged tweets: Time span, No. of Twitter users, No. of geotagged tweets,\n",
    "Days covered/user, No. of geotagged tweets/day/user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-69037101444b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m df = pd.DataFrame([load_region_tweets(region=x) for x in region_list],\n\u001B[0m\u001B[0;32m      2\u001B[0m                   columns=('region', 'time_span', 'num_users', 'num_geo', 'num_days', 'num_geo_freq'))\n\u001B[0;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'gdp_capita'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gdp_capita'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'country'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'country'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'pop'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'pop'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-6-69037101444b>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m df = pd.DataFrame([load_region_tweets(region=x) for x in region_list],\n\u001B[0m\u001B[0;32m      2\u001B[0m                   columns=('region', 'time_span', 'num_users', 'num_geo', 'num_days', 'num_geo_freq'))\n\u001B[0;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'gdp_capita'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gdp_capita'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'country'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'country'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'pop'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'region'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mregion_manager\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'pop'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-b5f9aaf59d46>\u001B[0m in \u001B[0;36mload_region_tweets\u001B[1;34m(region)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mnum_users\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muserid\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[0mnum_geo\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m     \u001B[0mnum_days\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmedian\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'userid'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'day'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnunique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m     \u001B[0mnum_geo_freq\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmedian\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'userid'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'userid'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'day'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnunique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mregion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime_span\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_users\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_geo\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_days\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_geo_freq\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\groupby\\generic.py\u001B[0m in \u001B[0;36mnunique\u001B[1;34m(self, dropna)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    631\u001B[0m         \u001B[0mcodes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0malgorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfactorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msort\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 632\u001B[1;33m         \u001B[0msorter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlexsort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcodes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    633\u001B[0m         \u001B[0mcodes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcodes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msorter\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    634\u001B[0m         \u001B[0mids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mids\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msorter\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mlexsort\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([load_region_tweets(region=x) for x in region_list],\n",
    "                  columns=('region', 'time_span', 'num_users', 'num_geo', 'num_days', 'num_geo_freq'))\n",
    "df.loc[:, 'gdp_capita'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['gdp_capita'])\n",
    "df.loc[:, 'country'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['country'])\n",
    "df.loc[:, 'pop'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['pop'])\n",
    "df.loc[:, 'time_span'] = df.loc[:, 'time_span'].apply(lambda x: ' - '.join([x_t.split(' ')[0] for x_t in x.split(' - ')]))\n",
    "df.loc[:, 'region'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['name'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_clipboard(index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1-extra Summarize the geotagged tweets used as input to the model - by user\n",
    "This is for dissertation presentation - sparsity issue.\n",
    "\n",
    "Geotagged tweets: Time span, No. of Twitter users, No. of geotagged tweets,\n",
    "Days covered/user, No. of geotagged tweets/day/user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sweden: 100%|██████████| 3961/3961 [00:05<00:00, 778.29it/s]\n",
      "netherlands: 100%|██████████| 5375/5375 [00:06<00:00, 793.24it/s]\n",
      "saopaulo: 100%|██████████| 10943/10943 [00:14<00:00, 766.58it/s] \n",
      "australia: 100%|██████████| 3310/3310 [00:04<00:00, 756.92it/s]\n",
      "austria: 100%|██████████| 729/729 [00:00<00:00, 820.02it/s]\n",
      "barcelona: 100%|██████████| 1891/1891 [00:02<00:00, 728.01it/s]\n",
      "capetown: 100%|██████████| 1092/1092 [00:01<00:00, 760.97it/s]\n",
      "cebu: 100%|██████████| 1486/1486 [00:01<00:00, 754.36it/s]\n",
      "egypt: 100%|██████████| 1464/1464 [00:01<00:00, 779.08it/s]\n",
      "guadalajara: 100%|██████████| 684/684 [00:00<00:00, 767.69it/s]\n",
      "jakarta: 100%|██████████| 13088/13088 [00:17<00:00, 754.31it/s]\n",
      "johannesburg: 100%|██████████| 1268/1268 [00:01<00:00, 820.29it/s]\n",
      "kualalumpur: 100%|██████████| 4663/4663 [00:05<00:00, 838.44it/s]\n",
      "lagos: 100%|██████████| 812/812 [00:01<00:00, 795.90it/s]\n",
      "madrid: 100%|██████████| 3172/3172 [00:03<00:00, 868.21it/s]\n",
      "manila: 100%|██████████| 11997/11997 [00:14<00:00, 817.76it/s]\n",
      "mexicocity: 100%|██████████| 15615/15615 [00:18<00:00, 822.62it/s]\n",
      "moscow: 100%|██████████| 4206/4206 [00:05<00:00, 809.16it/s]\n",
      "nairobi: 100%|██████████| 644/644 [00:00<00:00, 879.42it/s]\n",
      "rio: 100%|██████████| 6063/6063 [00:07<00:00, 824.33it/s]\n",
      "saudiarabia: 100%|██████████| 3117/3117 [00:03<00:00, 869.21it/s]\n",
      "stpertersburg: 100%|██████████| 1386/1386 [00:01<00:00, 839.37it/s]\n",
      "surabaya: 100%|██████████| 2414/2414 [00:02<00:00, 807.53it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([region_tweets_stats_per_user(region=x) for x in region_list])\n",
    "df.loc[:, 'gdp_capita'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['gdp_capita'])\n",
    "df.loc[:, 'country'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['country'])\n",
    "df.loc[:, 'pop'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['pop'])\n",
    "df.loc[:, 'region'] = df.loc[:, 'region'].apply(lambda x: region_manager[x]['name'])\n",
    "df.to_csv(f'../../dbs/regional_stats.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Merge ODMs for visualisation\n",
    "This part applies to Sweden, The Netherlands, and Sao Paulo, Brazil.\n",
    "\n",
    "Separate files will be deleted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for region in ['sweden', 'netherlands', 'saopaulo']:\n",
    "    df = pd.read_csv(f'../../dbs/{region}/odm_gt.csv')\n",
    "    df_c = pd.read_csv(f'../../dbs/{region}/odm_calibration.csv')\n",
    "    df_v = pd.read_csv(f'../../dbs/{region}/odm_validation.csv')\n",
    "    df_cb = pd.read_csv(f'../../dbs/{region}/odm_benchmark_c.csv')\n",
    "    df_vb = pd.read_csv(f'../../dbs/{region}/odm_benchmark_v.csv')\n",
    "    df = pd.merge(df, df_c, on=['ozone', 'dzone'])\n",
    "    df = df.rename(columns={'model': 'model_c'})\n",
    "    df = pd.merge(df, df_v, on=['ozone', 'dzone'])\n",
    "    df = df.rename(columns={'model': 'model_v'})\n",
    "    df = pd.merge(df, df_cb, on=['ozone', 'dzone'])\n",
    "    df = df.rename(columns={'benchmark': 'benchmark_c'})\n",
    "    df = pd.merge(df, df_vb, on=['ozone', 'dzone'])\n",
    "    df = df.rename(columns={'benchmark': 'benchmark_v'})\n",
    "    df.loc[:, ['ozone', 'dzone',\n",
    "               'gt', 'model_c', 'model_v',\n",
    "               'benchmark_c', 'benchmark_v']].to_csv(f'../../dbs/{region}/odms.csv', index=False)\n",
    "    os.remove(f'../../dbs/{region}/odm_gt.csv')\n",
    "    os.remove(f'../../dbs/{region}/odm_calibration.csv')\n",
    "    os.remove(f'../../dbs/{region}/odm_validation.csv')\n",
    "    os.remove(f'../../dbs/{region}/odm_benchmark_c.csv')\n",
    "    os.remove(f'../../dbs/{region}/odm_benchmark_v.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Quantify the od-pair similarity\n",
    "This part applies to Sweden, The Netherlands, and Sao Paulo, Brazil.\n",
    "\n",
    "The overall similarity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "         region       type data       cor              p\n0        sweden      model    c  0.187270  6.653740e-246\n1        sweden  benchmark    c  0.220507  9.577697e-277\n2        sweden      model    v  0.184070  2.009274e-128\n3        sweden  benchmark    v  0.266851  3.416318e-215\n4   netherlands      model    c  0.439008   0.000000e+00\n5   netherlands  benchmark    c  0.331307   0.000000e+00\n6   netherlands      model    v  0.432798   0.000000e+00\n7   netherlands  benchmark    v  0.404650   0.000000e+00\n8      saopaulo      model    c  0.414933   0.000000e+00\n9      saopaulo  benchmark    c  0.296695   0.000000e+00\n10     saopaulo      model    v  0.480378   0.000000e+00\n11     saopaulo  benchmark    v  0.356082   0.000000e+00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>type</th>\n      <th>data</th>\n      <th>cor</th>\n      <th>p</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sweden</td>\n      <td>model</td>\n      <td>c</td>\n      <td>0.187270</td>\n      <td>6.653740e-246</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sweden</td>\n      <td>benchmark</td>\n      <td>c</td>\n      <td>0.220507</td>\n      <td>9.577697e-277</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sweden</td>\n      <td>model</td>\n      <td>v</td>\n      <td>0.184070</td>\n      <td>2.009274e-128</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sweden</td>\n      <td>benchmark</td>\n      <td>v</td>\n      <td>0.266851</td>\n      <td>3.416318e-215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>netherlands</td>\n      <td>model</td>\n      <td>c</td>\n      <td>0.439008</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>netherlands</td>\n      <td>benchmark</td>\n      <td>c</td>\n      <td>0.331307</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>netherlands</td>\n      <td>model</td>\n      <td>v</td>\n      <td>0.432798</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>netherlands</td>\n      <td>benchmark</td>\n      <td>v</td>\n      <td>0.404650</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>saopaulo</td>\n      <td>model</td>\n      <td>c</td>\n      <td>0.414933</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>saopaulo</td>\n      <td>benchmark</td>\n      <td>c</td>\n      <td>0.296695</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>saopaulo</td>\n      <td>model</td>\n      <td>v</td>\n      <td>0.480378</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>saopaulo</td>\n      <td>benchmark</td>\n      <td>v</td>\n      <td>0.356082</td>\n      <td>0.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_list = []\n",
    "for region in ['sweden', 'netherlands', 'saopaulo']:\n",
    "    df = pd.read_csv(f'../../dbs/{region}/odms.csv')\n",
    "    df_c = df.loc[(df.gt != 0) & (df.model_c != 0) & (df.benchmark_c != 0), :]\n",
    "    mc = stats.kendalltau(df_c.loc[:, 'gt'], df_c.loc[:, 'model_c'])\n",
    "    quant_list.append((region, 'model', 'c', mc.correlation, mc.pvalue))\n",
    "\n",
    "    bc = stats.kendalltau(df_c.loc[:, 'gt'], df_c.loc[:, 'benchmark_c'])\n",
    "    quant_list.append((region, 'benchmark', 'c', bc.correlation, bc.pvalue))\n",
    "\n",
    "    df_v = df.loc[(df.gt != 0) & (df.model_v != 0) & (df.benchmark_v != 0), :]\n",
    "    mv = stats.kendalltau(df_v.loc[:, 'gt'], df_v.loc[:, 'model_v'])\n",
    "    quant_list.append((region, 'model', 'v', mv.correlation, mv.pvalue))\n",
    "\n",
    "    bv = stats.kendalltau(df_v.loc[:, 'gt'], df_v.loc[:, 'benchmark_v'])\n",
    "    quant_list.append((region, 'benchmark', 'v', bv.correlation, bv.pvalue))\n",
    "df_stats = pd.DataFrame(quant_list, columns=['region', 'type', 'data', 'cor', 'p'])\n",
    "df_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "region       type     \nnetherlands  benchmark    0.367978\n             model        0.435903\nsaopaulo     benchmark    0.326389\n             model        0.447656\nsweden       benchmark    0.243679\n             model        0.185670\nName: cor, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats.groupby(['region', 'type'])['cor'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Check the total PKT of The Netherlands using Survey"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sheet1 = pd.read_excel(\"../../dbs/netherlands/mobility_data/OViN2017_Databestand.xlsx\")\n",
    "trips = sheet1.loc[:, ['OPID', 'AfstV', 'FactorV']]\n",
    "trips = trips.rename(columns={\n",
    "    'OPID': 'userid',\n",
    "    'AfstV': 'distance',\n",
    "    'FactorV': 'weight_trip'\n",
    "})\n",
    "# Prepare the actual trip distances\n",
    "trips_d = trips.dropna(subset=['distance'])\n",
    "trips_d.loc[:, 'distance'] = trips_d.loc[:, 'distance'] / 10 # hectometer to km"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trips_capita = pd.DataFrame(trips_d.groupby('userid')['distance'].sum() * 365 / 1000,\n",
    "                            columns=['distance']).reset_index() # 1000 km\n",
    "trips_capita = pd.merge(trips_capita, trips_d.loc[:, ['userid', 'weight_trip']].drop_duplicates(subset=['weight_trip']), on='userid')\n",
    "trips_capita.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import weightedstats as ws\n",
    "ws.numpy_weighted_median(trips_capita['distance'], weights=trips_capita['weight_trip']), \\\n",
    "np.average(trips_capita['distance'], weights=trips_capita['weight_trip'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5-extra Temporal profile of geotagged tweets\n",
    "Sweden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "region = 'sweden'\n",
    "time_list = [(w, h) for w in range(0, 7) for h in range(0, 24)]\n",
    "time_dict = {x: y for x,y in zip(time_list, range(0, len(time_list)))}\n",
    "df_gt = pd.read_csv(f'../../dbs/{region}/survey_deso/desti_temporal_survey.csv')\n",
    "df_gt['count'] = df_gt['count'] / df_gt['count'].sum() * 100\n",
    "df_gt.loc[:, 'seq'] = df_gt.apply(lambda row: time_dict[(row['weekday'], row['hour'])], axis=1)\n",
    "df_gt.loc[:, 'source'] = 'Travel survey'\n",
    "\n",
    "df_tw = pd.read_csv(f'../../dbs/{region}/geotweets.csv')\n",
    "df_tw['createdat'] = pd.to_datetime(df_tw['createdat'], infer_datetime_format=True)\n",
    "df_tw['weekday'] = df_tw['createdat'].apply(lambda x: x.weekday())\n",
    "df_tw['hour'] = df_tw['createdat'].apply(lambda x: x.hour)\n",
    "df_tw_tempo = df_tw.groupby(['weekday', 'hour']).size().reset_index()\n",
    "df_tw_tempo.rename(columns={0: 'count'}, inplace=True)\n",
    "df_tw_tempo['count'] = df_tw_tempo['count'] / df_tw_tempo['count'].sum() * 100\n",
    "df_tw_tempo.loc[:, 'seq'] = df_tw_tempo.apply(lambda row: time_dict[(row['weekday'], row['hour'])], axis=1)\n",
    "df_tw_tempo.loc[:, 'source'] = 'Geotagged tweets'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "pd.concat([df_tw_tempo, df_gt]).to_csv(f'../../dbs/{region}/temporal_tw_vs_survey.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}